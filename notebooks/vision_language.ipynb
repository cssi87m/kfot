{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Import essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/quangminh/Documents/code/Python/kfot/notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/quangminh/Documents/code/Python/kfot\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quangminh/anaconda3/envs/OT-CLIP/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPModel,CLIPProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "import ot\n",
    "from random import randint\n",
    "\n",
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from optimal_transport.experiments.vision_language.datasets import *\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Embedding images and texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the transformations\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.RandomHorizontalFlip(),   # Randomly flip the images horizontally for data augmentation\n",
    "#     transforms.RandomCrop(32, padding=4), # Randomly crop the image with padding\n",
    "#     transforms.ToTensor(),                # Convert PIL image to PyTorch Tensor\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) # Normalize with mean and std\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_data = cifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [14:41<00:00, 88.15s/it]\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(test_data.classes)\n",
    "n_samples = len(test_data)  # total number of samples\n",
    "res_img = torch.empty((n_samples, 512))  # Preallocate memory\n",
    "res_text = torch.empty((n_samples, 512))\n",
    "index = 0\n",
    "for class_name in tqdm(test_data.classes):\n",
    "    # Get batched data for the current class\n",
    "    loader = test_data.get_batch_loader(class_name, batch_size=32)\n",
    "    for batch in loader:\n",
    "        images, labels, _ = batch\n",
    "        # Preprocess images and text labels\n",
    "        inputs = processor(text=labels, images=images, return_tensors='pt', padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            images_embeds = outputs.image_embeds  # embeddings for this batch\n",
    "            text_embeds = outputs.text_embeds\n",
    "        # Save the embeddings to the pre-allocated tensor\n",
    "        batch_size = images_embeds.size(0)\n",
    "        res_img[index:index + batch_size] = images_embeds\n",
    "        res_text[index:index + batch_size] = text_embeds\n",
    "        index += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cifar10/embedded/cifar10-image-embeds.pt','wb') as f:\n",
    "    torch.save(res_img,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cifar10/embedded/cifar10-text-embeds.pt','wb') as f:\n",
    "    torch.save(res_text,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "img_embedding, text_embedding = torch.load('data/cifar10/embedded/cifar10-image-embeds.pt'), torch.load('data/cifar10/embedded/cifar10-text-embeds.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0336, -0.0199, -0.0080,  ...,  0.0675, -0.0410, -0.0146],\n",
      "        [ 0.0287,  0.0071,  0.0077,  ...,  0.0560,  0.0004,  0.0079],\n",
      "        [-0.0079, -0.0167, -0.0181,  ...,  0.0759,  0.0036, -0.0121],\n",
      "        ...,\n",
      "        [ 0.0153, -0.0238, -0.0121,  ...,  0.0715,  0.0095,  0.0191],\n",
      "        [ 0.0240, -0.0024, -0.0517,  ...,  0.0776, -0.0176,  0.0149],\n",
      "        [-0.0143,  0.0002, -0.0143,  ...,  0.0400, -0.0037,  0.0150]])\n",
      "tensor([[ 0.0058,  0.0009,  0.0031,  ...,  0.0072, -0.0021, -0.0193],\n",
      "        [ 0.0058,  0.0009,  0.0031,  ...,  0.0072, -0.0021, -0.0193],\n",
      "        [ 0.0058,  0.0009,  0.0031,  ...,  0.0072, -0.0021, -0.0193],\n",
      "        ...,\n",
      "        [ 0.0064,  0.0218, -0.0012,  ..., -0.0359, -0.0177,  0.0077],\n",
      "        [ 0.0064,  0.0218, -0.0012,  ..., -0.0359, -0.0177,  0.0077],\n",
      "        [ 0.0064,  0.0218, -0.0012,  ..., -0.0359, -0.0177,  0.0077]])\n"
     ]
    }
   ],
   "source": [
    "print(img_embedding) \n",
    "print(text_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Test zero-shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from optimal_transport.adapters.info import InfoOT\n",
    "from optimal_transport.adapters.emd import EMD\n",
    "from optimal_transport.adapters.entropic import EntropicOT\n",
    "from optimal_transport.adapters.low_rank import LrSinkhornOT\n",
    "from optimal_transport.adapters.kpg.kp import KPGRLKP\n",
    "from optimal_transport.adapters.factored.fot import FOT\n",
    "from optimal_transport.adapters.factored.kfot import KFOT\n",
    "from optimal_transport.adapters.factored.lot import LOT\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:20<00:00, 20.72s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m P\n\u001b[1;32m     27\u001b[0m transport_plan \u001b[38;5;241m=\u001b[39m fgwi_softmax(img_embedding, text_embedding, \u001b[38;5;241m1e-4\u001b[39m,\u001b[38;5;241m1e-4\u001b[39m,\u001b[38;5;241m0.05\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m res \u001b[38;5;241m=\u001b[39m [test_data\u001b[38;5;241m.\u001b[39mclasses[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtransport_plan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Fused-Gromov softmax\n",
    "def fgwi_softmax(image_features, text_features, a1, a2, reg, n):\n",
    "    \"\"\"\n",
    "    image_features: image features\n",
    "    text_features: text features\n",
    "    a1: regularization term for scaling the KL divergence term 1\n",
    "    a2: regularization term for scaling the KL divergence term 2\n",
    "    reg: regularization parameter > 0\n",
    "    n: number of iterations for adjusting the structural cost matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # KL term 1: compute the self-similarity matrix for image features\n",
    "    sigma_1 = image_features @ image_features.t()\n",
    "    # KL term 2: compute the self-similarity matrix for text features\n",
    "    sigma_2 = text_features @ text_features.t()\n",
    "\n",
    "    # init cost matrix\n",
    "    C = 1.0 - image_features @ text_features.t()\n",
    "    # init transport plan\n",
    "    P = F.softmax(-C / reg)\n",
    "\n",
    "    for _ in tqdm(range(n)):\n",
    "        C = C - a1 * sigma_1 @ P @ sigma_2 * a2\n",
    "        P = F.softmax(-C/ reg)\n",
    "    return P\n",
    "\n",
    "transport_plan = fgwi_softmax(img_embedding, text_embedding, 1e-4,1e-4,0.05,100)\n",
    "res = [test_data.targets[x] for x in transport_plan.argmax(1)]\n",
    "\n",
    "print(accuracy_score(test_data.targets,res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1024\n"
     ]
    }
   ],
   "source": [
    "ot = LrSinkhornOT()\n",
    "ot = ot.fit(img_embedding.numpy(),text_embedding.numpy(),np.ones(img_embedding.shape[0])/img_embedding.shape[0],np.ones(text_embedding.shape[0])/text_embedding.shape[0])\n",
    "res = [test_data.targets[x] for x in ot.P_.argmax(1)]\n",
    "\n",
    "print(accuracy_score(test_data.targets,res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m ot \u001b[38;5;241m=\u001b[39m EntropicOT()\n\u001b[1;32m      2\u001b[0m ot \u001b[38;5;241m=\u001b[39m ot\u001b[38;5;241m.\u001b[39mfit(img_embedding\u001b[38;5;241m.\u001b[39mnumpy(),text_embedding\u001b[38;5;241m.\u001b[39mnumpy(),np\u001b[38;5;241m.\u001b[39mones(img_embedding\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m/\u001b[39mimg_embedding\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],np\u001b[38;5;241m.\u001b[39mones(text_embedding\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m/\u001b[39mtext_embedding\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m res \u001b[38;5;241m=\u001b[39m [\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ot\u001b[38;5;241m.\u001b[39mP_\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy_score(test_data\u001b[38;5;241m.\u001b[39mtargets,res))\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ot = EntropicOT()\n",
    "ot = ot.fit(img_embedding.numpy(),text_embedding.numpy(), np.ones(img_embedding.shape[0])/img_embedding.shape[0], np.ones(text_embedding.shape[0])/text_embedding.shape[0])\n",
    "res = [test_data.targets[x] for x in ot.P_.argmax(1)]\n",
    "\n",
    "print(accuracy_score(test_data.targets,res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1054\n"
     ]
    }
   ],
   "source": [
    "res = [test_data.targets[x] for x in ot.P_.argmax(1)]\n",
    "\n",
    "print(accuracy_score(test_data.targets,res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot = InfoOT(num_iters=10)\n",
    "ot = ot.fit(img_embedding.numpy(),text_embedding.numpy(),np.ones(img_embedding.shape[0])/img_embedding.shape[0],np.ones(text_embedding.shape[0])/text_embedding.shape[0])\n",
    "res = [test_data.targets[x] for x in ot.P_.argmax(1)]\n",
    "\n",
    "print(accuracy_score(test_data.targets,res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot = EMD(solver_kwargs={\"maxItermax\": 10000})\n",
    "ot = ot.fit(img_embedding.numpy(),text_embedding.numpy(),np.ones(img_embedding.shape[0])/img_embedding.shape[0],np.ones(text_embedding.shape[0])/text_embedding.shape[0])\n",
    "res = [test_data.targets[x] for x in ot.P_.argmax(1)]\n",
    "\n",
    "print(accuracy_score(test_data.targets,res))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OT-CLIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
